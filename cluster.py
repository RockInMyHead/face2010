import os
import cv2
import shutil
import numpy as np
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from sklearn.metrics.pairwise import cosine_distances, euclidean_distances
from sklearn.cluster import DBSCAN
import face_recognition
import hdbscan
from collections import defaultdict

IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}

def is_image(p: Path) -> bool:
    return p.suffix.lower() in IMG_EXTS

def _win_long(path: Path) -> str:
    p = str(path.resolve())
    if os.name == "nt":
        return "\\\\?\\" + p if not p.startswith("\\\\?\\") else p
    return p

def imread_safe(path: Path):
    try:
        data = np.fromfile(_win_long(path), dtype=np.uint8)
        if data.size == 0:
            return None
        return cv2.imdecode(data, cv2.IMREAD_COLOR)
    except Exception:
        return None

def merge_clusters_by_centroid(
    embeddings: List[np.ndarray],
    owners: List[Path],
    raw_labels: np.ndarray,
    threshold: Optional[float] = None,
    auto_threshold: bool = False,
    margin: float = 0.05,
    min_threshold: float = 0.2,
    max_threshold: float = 0.4,
    progress_callback=None
) -> Tuple[Dict[int, Set[Path]], Dict[Path, Set[int]]]:

    if progress_callback:
        progress_callback("üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–ª–∏–∑–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 92)

    cluster_embeddings: Dict[int, List[np.ndarray]] = defaultdict(list)
    cluster_paths: Dict[int, List[Path]] = defaultdict(list)

    for label, emb, path in zip(raw_labels, embeddings, owners):
        if label == -1:
            continue
        cluster_embeddings[label].append(emb)
        cluster_paths[label].append(path)

    centroids = {label: np.mean(embs, axis=0) for label, embs in cluster_embeddings.items()}
    labels = list(centroids.keys())

    if auto_threshold and threshold is None:
        pairwise = [cosine_distances([centroids[a]], [centroids[b]])[0][0]
                    for i, a in enumerate(labels) for b in labels[i+1:]]
        if pairwise:
            mean_dist = np.mean(pairwise)
            threshold = max(min_threshold, min(mean_dist - margin, max_threshold))
        else:
            threshold = min_threshold

        if progress_callback:
            progress_callback(f"üìè –ê–≤—Ç–æ-–ø–æ—Ä–æ–≥ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {threshold:.3f}", 93)
    elif threshold is None:
        threshold = 0.3

    next_cluster_id = 0
    label_to_group = {}
    total = len(labels)

    for i, label_i in enumerate(labels):
        if progress_callback:
            percent = 93 + int((i + 1) / max(total, 1) * 2)
            progress_callback(f"üîÅ –°–ª–∏—è–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {percent}% ({i+1}/{total})", percent)

        if label_i in label_to_group:
            continue
        group = [label_i]
        for j in range(i + 1, len(labels)):
            label_j = labels[j]
            if label_j in label_to_group:
                continue
            dist = cosine_distances([centroids[label_i]], [centroids[label_j]])[0][0]
            if dist < threshold:
                group.append(label_j)

        for l in group:
            label_to_group[l] = next_cluster_id
        next_cluster_id += 1

    merged_clusters: Dict[int, Set[Path]] = defaultdict(set)
    cluster_by_img: Dict[Path, Set[int]] = defaultdict(set)

    for label, path in zip(raw_labels, owners):
        if label == -1:
            continue
        new_label = label_to_group[label]
        merged_clusters[new_label].add(path)
        cluster_by_img[path].add(new_label)

    return merged_clusters, cluster_by_img

def find_optimal_epsilon(distance_matrix):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π epsilon –¥–ª—è DBSCAN —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ k-distance –≥—Ä–∞—Ñ–∏–∫–∞.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç 75-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞.
    """
    distances = []
    for i in range(len(distance_matrix)):
        row = distance_matrix[i]
        sorted_dist = np.sort(row)
        if len(sorted_dist) > 1:
            # –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞ (–∏—Å–∫–ª—é—á–∞—è —Å–µ–±—è)
            distances.append(sorted_dist[1])
    
    if not distances:
        return 0.6  # –ó–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    # 75-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –¥–ª—è —Å—Ç—Ä–æ–≥–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    epsilon = np.percentile(distances, 75)
    return epsilon

def verify_cluster_similarity(cluster_paths, threshold=0.6):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —á—Ç–æ –≤—Å–µ –ª–∏—Ü–∞ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞—Ç –æ–¥–Ω–æ–º—É —á–µ–ª–æ–≤–µ–∫—É.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–µ—Ä–µ–∑ face_recognition.
    
    Args:
        cluster_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
        threshold: –ü–æ—Ä–æ–≥ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è (–º–µ–Ω—å—à–µ = —Å—Ç—Ä–æ–∂–µ)
    
    Returns:
        True –µ—Å–ª–∏ –≤—Å–µ –ª–∏—Ü–∞ –ø–æ—Ö–æ–∂–∏, False –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–∞–∑–Ω—ã–µ –ª—é–¥–∏
    """
    if len(cluster_paths) < 2:
        return True
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ–æ—Ç–æ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    embeddings = []
    valid_paths = []
    
    for path in cluster_paths:
        try:
            img = face_recognition.load_image_file(str(path))
            face_encodings = face_recognition.face_encodings(img, model="large")
            
            if face_encodings:
                embeddings.append(face_encodings[0])
                valid_paths.append(path)
        except:
            continue
    
    if len(embeddings) < 2:
        return True
    
    # –ü–æ–ø–∞—Ä–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ª–∏—Ü
    for i in range(len(embeddings)):
        for j in range(i + 1, len(embeddings)):
            distance = np.linalg.norm(embeddings[i] - embeddings[j])
            
            # –ï—Å–ª–∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –±–æ–ª—å—à–µ –ø–æ—Ä–æ–≥–∞ - —Ä–∞–∑–Ω—ã–µ –ª—é–¥–∏
            if distance > threshold:
                print(f"  ‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —Ä–∞–∑–Ω—ã–µ –ª—é–¥–∏: {valid_paths[i].name} –∏ {valid_paths[j].name} (distance: {distance:.3f})")
                return False
    
    return True

def build_plan_live(
    input_dir: Path,
    det_size=(640, 640),
    min_score: float = 0.95,  # –ü–æ–≤—ã—à–µ–Ω –¥–ª—è –ª—É—á—à–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    min_cluster_size: int = 1,  # –†–∞–∑—Ä–µ—à–∏—Ç—å –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Ñ–æ—Ç–æ
    min_samples: int = 1,  # –†–∞–∑—Ä–µ—à–∏—Ç—å –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    providers: List[str] = ("CPUExecutionProvider",),  # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    progress_callback=None,
    include_excluded: bool = False,
):
    print(f"üîç [CLUSTER] build_plan_live –≤—ã–∑–≤–∞–Ω–∞: input_dir={input_dir}, include_excluded={include_excluded}")
    
    input_dir = Path(input_dir)
    print(f"üîç [CLUSTER] input_dir –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω –≤ Path: {input_dir}")

    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, —É—á–∏—Ç—ã–≤–∞–µ–º —Ñ–ª–∞–≥ include_excluded
    excluded_names = ["–æ–±—â–∏–µ", "–æ–±—â–∞—è", "common", "shared", "–≤—Å–µ", "all", "mixed", "—Å–º–µ—à–∞–Ω–Ω—ã–µ"]
    print(f"üîç [CLUSTER] excluded_names: {excluded_names}")

    # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —Ç–∞–π–º–∞—É—Ç–∞
    import time
    start_time = time.time()
    max_processing_time = 300  # 5 –º–∏–Ω—É—Ç
    
    if include_excluded:
        # –í–∫–ª—é—á–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –¥–∞–∂–µ –∏–∑ –ø–∞–ø–æ–∫ "–æ–±—â–∏–µ"
        all_images = [
            p for p in input_dir.rglob("*")
            if is_image(p)
        ]
    else:
        # –ò—Å–∫–ª—é—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –ø–∞–ø–æ–∫ —Å –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∏–º–µ–Ω–∞–º–∏
        all_images = [
            p for p in input_dir.rglob("*")
            if is_image(p)
            and not any(ex in str(p).lower() for ex in excluded_names)
        ]

    print(f"üîç build_plan_live: input_dir={input_dir}, include_excluded={include_excluded}, –Ω–∞–π–¥–µ–Ω–æ {len(all_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    if len(all_images) > 0:
        print(f"üîç –ü–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–≤: {[str(p) for p in all_images[:3]]}")
    
    if progress_callback:
        progress_callback(f"üìÇ –°–∫–∞–Ω–∏—Ä—É–µ—Ç—Å—è: {input_dir}, –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(all_images)}", 1)

    # face_recognition –∏—Å–ø–æ–ª—å–∑—É–µ—Ç dlib - –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    if progress_callback:
        progress_callback("‚úÖ –ú–æ–¥–µ–ª—å face_recognition –≥–æ—Ç–æ–≤–∞ (dlib + CNN), –Ω–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑...", 10)

    embeddings = []
    owners = []
    img_face_count = {}
    unreadable = []
    no_faces = []

    total = len(all_images)
    processed_faces = 0
    
    print(f"üîç [CLUSTER] –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É {total} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å face_recognition (–º–æ–¥–µ–ª—å: large CNN)")
    
    for i, p in enumerate(all_images):
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â–∏–π —Ç–∞–π–º–∞—É—Ç
        if time.time() - start_time > max_processing_time:
            print(f"‚è∞ –û–±—â–∏–π —Ç–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–µ–≤—ã—à–µ–Ω ({max_processing_time}—Å), –ø—Ä–µ—Ä—ã–≤–∞–µ–º")
            break
            
        # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if progress_callback:
            percent = 10 + int((i + 1) / max(total, 1) * 70)  # 10-80% –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            progress_callback(f"üì∑ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {percent}% ({i+1}/{total}) - {p.name}", percent)
        
        # –ó–∞—â–∏—Ç–∞ –æ—Ç –∑–∞–≤–∏—Å–∞–Ω–∏—è: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –±–æ–ª—å—à–µ 50MB
        try:
            file_size = p.stat().st_size
            if file_size > 50 * 1024 * 1024:  # 50MB
                print(f"  ‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª {p.name} ({file_size // (1024*1024)}MB)")
                unreadable.append(p)
                continue
        except:
            pass
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–µ—Ä–µ–∑ face_recognition
            img = face_recognition.load_image_file(str(p))
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: —Å–∂–∏–º–∞–µ–º –±–æ–ª—å—à–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if img.shape[0] > 1200 or img.shape[1] > 1200:
                # –°–∂–∏–º–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –¥–æ —Ä–∞–∑—É–º–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                scale = min(1200 / img.shape[0], 1200 / img.shape[1])
                new_height = int(img.shape[0] * scale)
                new_width = int(img.shape[1] * scale)
                img = cv2.resize(img, (new_width, new_height))
                print(f"  üìè –°–∂–∞—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {p.name}: {img.shape}")
            
            # –ù–∞—Ö–æ–¥–∏–º –ª–∏—Ü–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º HOG –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏, CNN —Ç–æ–ª—å–∫–æ –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤)
            # HOG –±—ã—Å—Ç—Ä–µ–µ –≤ 10-20 —Ä–∞–∑, –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–µ–Ω
            face_locations = face_recognition.face_locations(img, model="hog")
            
            # –ï—Å–ª–∏ HOG –Ω–µ –Ω–∞—à–µ–ª –ª–∏—Ü, –ø—Ä–æ–±—É–µ–º CNN (–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏)
            if not face_locations:
                try:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏ –æ–±—â–∏–π —Ç–∞–π–º–∞—É—Ç
                    if time.time() - start_time > max_processing_time - 30:  # –û—Å—Ç–∞–≤–ª—è–µ–º 30—Å –Ω–∞ –æ—Å—Ç–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
                        print(f"  ‚è∞ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º CNN –¥–ª—è {p.name} - –º–∞–ª–æ –≤—Ä–µ–º–µ–Ω–∏")
                        face_locations = []
                    else:
                        # –ü—Ä–æ–±—É–µ–º CNN —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –≤—Ä–µ–º—è
                        cnn_start = time.time()
                        face_locations = face_recognition.face_locations(img, model="cnn")
                        cnn_time = time.time() - cnn_start
                        if cnn_time > 10:  # –ï—Å–ª–∏ CNN –∑–∞–Ω—è–ª –±–æ–ª—å—à–µ 10 —Å–µ–∫—É–Ω–¥
                            print(f"  ‚è∞ CNN –∑–∞–Ω—è–ª {cnn_time:.1f}—Å –¥–ª—è {p.name}")
                        
                except Exception as e:
                    print(f"  ‚ùå –û—à–∏–±–∫–∞ CNN –¥–ª—è {p.name}: {e}")
                    face_locations = []
            
            if not face_locations:
                no_faces.append(p)
                continue
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–º–æ–¥–µ–ª—å "large" –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏)
            face_encodings = face_recognition.face_encodings(
                img, 
                known_face_locations=face_locations,
                model="large"  # 128-–º–µ—Ä–Ω—ã–π –≤–µ–∫—Ç–æ—Ä, 99.38% —Ç–æ—á–Ω–æ—Å—Ç—å
            )
            
            if not face_encodings:
                no_faces.append(p)
                continue

            count = 0
            for emb in face_encodings:
                # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —É–∂–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –≤ face_recognition
                embeddings.append(emb.astype(np.float64))
                owners.append(p)
                count += 1
                processed_faces += 1

            if count > 0:
                img_face_count[p] = count
                
        except (TimeoutError, Exception) as e:
            if isinstance(e, TimeoutError):
                print(f"  ‚è∞ –¢–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ {p.name}: {e}")
            else:
                print(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {p.name}: {e}")
            unreadable.append(p)
            continue

    if not embeddings:
        if progress_callback:
            progress_callback("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ª–∏—Ü –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏", 100)
        print(f"‚ö†Ô∏è –ù–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {input_dir}")
        return {
            "clusters": {},
            "plan": [],
            "unreadable": [str(p) for p in unreadable],
            "no_faces": [str(p) for p in no_faces],
        }

    # –≠—Ç–∞–ø 2: –£–ª—É—á—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å HDBSCAN
    if progress_callback:
        progress_callback(f"üîÑ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è {len(embeddings)} –ª–∏—Ü —Å HDBSCAN...", 80)
    
    print(f"üîç [CLUSTER] –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
    
    X = np.vstack(embeddings)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è cosine distance
    from sklearn.preprocessing import normalize
    X_normalized = normalize(X, norm='l2')
    
    print(f"üîç [CLUSTER] –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –¥–ª—è cosine distance")
    
    if progress_callback:
        progress_callback("üîÑ HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è...", 85)
    
    # HDBSCAN - –±–æ–ª–µ–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —à—É–º
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=2,  # –ú–∏–Ω–∏–º—É–º 2 —Ñ–æ—Ç–æ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞
        min_samples=1,  # –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏
        metric='euclidean',  # Euclidean distance –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
        cluster_selection_method='eom',  # Excess of Mass - –ª—É—á—à–µ –¥–ª—è –ª–∏—Ü
        allow_single_cluster=False,  # –†–∞–∑—Ä–µ—à–∏—Ç—å –æ–¥–∏–Ω –±–æ–ª—å—à–æ–π –∫–ª–∞—Å—Ç–µ—Ä –µ—Å–ª–∏ –≤—Å–µ –ø–æ—Ö–æ–∂–∏
        prediction_data=True  # –í–∫–ª—é—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    )
    
    print(f"üîç [CLUSTER] –ó–∞–ø—É—Å–∫–∞–µ–º HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é...")
    raw_labels = clusterer.fit_predict(X_normalized)
    print(f"üîç [CLUSTER] HDBSCAN fit_predict –∑–∞–≤–µ—Ä—à–µ–Ω")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –æ–±—ä–µ–¥–∏–Ω—è–µ–º –±–ª–∏–∑–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã —á–µ—Ä–µ–∑ condensed tree
    probabilities = clusterer.probabilities_
    print(f"üîç [CLUSTER] HDBSCAN –Ω–∞—à–µ–ª {len(set(raw_labels)) - (1 if -1 in raw_labels else 0)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    print(f"üîç [CLUSTER] –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏: {np.mean(probabilities):.3f}")
    print(f"üîç [CLUSTER] –ù–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –ª–∏—Ü: {list(raw_labels).count(-1)}")
    
    # –≠—Ç–∞–ø 2.5: –£–º–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–ª–∏–∑–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    if progress_callback:
        progress_callback("üîÑ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–ª–∏–∑–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 87)
    
    print(f"üîç [CLUSTER] –ù–∞—á–∏–Ω–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –±–ª–∏–∑–∫–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
    # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–∏—Å–∫–ª—é—á–∞—è -1 - —à—É–º)
    unique_clusters = [l for l in set(raw_labels) if l != -1]
    print(f"üîç [CLUSTER] –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {len(unique_clusters)}")
    
    if len(unique_clusters) > 1:
        # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_centroids = {}
        for cluster_id in unique_clusters:
            mask = raw_labels == cluster_id
            cluster_centroids[cluster_id] = np.mean(X_normalized[mask], axis=0)
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞–º–∏
        from sklearn.metrics.pairwise import cosine_distances
        centroid_ids = list(cluster_centroids.keys())
        centroid_vectors = np.array([cluster_centroids[cid] for cid in centroid_ids])
        centroid_distances = cosine_distances(centroid_vectors)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã —Å —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º < 0.2 (–æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ)
        merge_threshold = 0.2  # Cosine distance < 0.2 = –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–∏–µ –ª–∏—Ü–∞
        # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞–º–∏
        from numpy import triu_indices
        dists = centroid_distances[triu_indices(len(centroid_ids), k=1)]
        if len(dists) > 0:
            print(f"üîç [CLUSTER] –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞–º–∏: –º–∏–Ω={dists.min():.3f}, —Å—Ä–µ–¥–Ω–µ–µ={dists.mean():.3f}, –º–∞–∫—Å={dists.max():.3f}")
        merged_labels = {}
        
        for i, cluster_i in enumerate(centroid_ids):
            if cluster_i in merged_labels:
                continue
            merged_labels[cluster_i] = cluster_i
            
            for j in range(i + 1, len(centroid_ids)):
                cluster_j = centroid_ids[j]
                if cluster_j in merged_labels:
                    continue
                    
                if centroid_distances[i][j] < merge_threshold:
                    merged_labels[cluster_j] = cluster_i
                    print(f"  üîó –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã {cluster_j} ‚Üí {cluster_i} (distance: {centroid_distances[i][j]:.3f})")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫ raw_labels
        print(f"üîç [CLUSTER] –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
        for idx in range(len(raw_labels)):
            if raw_labels[idx] in merged_labels:
                raw_labels[idx] = merged_labels[raw_labels[idx]]
        
        print(f"üîç [CLUSTER] –ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è: {len(set(raw_labels)) - (1 if -1 in raw_labels else 0)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
        # Fallback to DBSCAN if too few clusters
        merged_count = len([l for l in set(raw_labels) if l != -1])
        if merged_count <= 2:
            print(f"üîç [CLUSTER] –ú–∞–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ ({merged_count}), –ø—Ä–æ–±—É–µ–º DBSCAN fallback")
            from sklearn.cluster import DBSCAN
            from sklearn.metrics.pairwise import cosine_distances
            # –†–∞—Å—á–µ—Ç –º–∞—Ç—Ä–∏—Ü—ã —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π
            dist_matrix = cosine_distances(X_normalized)
            # –ü–æ–¥–±–æ—Ä epsilon
            eps = find_optimal_epsilon(dist_matrix)
            print(f"üîç [CLUSTER] DBSCAN eps={eps:.3f}")
            db = DBSCAN(eps=eps, min_samples=1, metric='precomputed')
            raw_labels = db.fit_predict(dist_matrix)
            print(f"üîç [CLUSTER] DBSCAN –Ω–∞—à–µ–ª {len(set(raw_labels)) - (1 if -1 in raw_labels else 0)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
            # –û–±–Ω–æ–≤–∏–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–∫ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å —Å–æ–∑–¥–∞–Ω–∏–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã—Ö –ø–∞–ø–æ–∫ –¥–ª—è –æ–¥–∏–Ω–æ—á–Ω—ã—Ö —Ñ–æ—Ç–æ
    print(f"üîç [CLUSTER] –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
    cluster_map = defaultdict(set)
    cluster_by_img = defaultdict(set)
    
    # –ù–∞—Ö–æ–¥–∏–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π label –¥–ª—è –Ω–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö
    max_label = max(raw_labels) if len(raw_labels) > 0 and max(raw_labels) >= 0 else -1
    next_single_label = max_label + 1
    print(f"üîç [CLUSTER] –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π label: {max_label}, —Å–ª–µ–¥—É—é—â–∏–π –æ–¥–∏–Ω–æ—á–Ω—ã–π: {next_single_label}")
    
    for idx, (label, path) in enumerate(zip(raw_labels, owners)):
        if label == -1:
            # –°–æ–∑–¥–∞—ë–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–µ–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–∏—Ü–∞
            unique_label = next_single_label
            cluster_map[unique_label].add(path)
            cluster_by_img[path].add(unique_label)
            next_single_label += 1
            print(f"  üìÅ –°–æ–∑–¥–∞–Ω –æ–¥–∏–Ω–æ—á–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä {unique_label} –¥–ª—è {path.name}")
        else:
            cluster_map[label].add(path)
            cluster_by_img[path].add(label)

    # –≠—Ç–∞–ø 3: –ú—è–≥–∫–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    if progress_callback:
        progress_callback("üîÑ –§–∏–Ω–∞–ª—å–Ω–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...", 90)
    
    print(f"üîç [CLUSTER] –ù–∞—á–∏–Ω–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é {len(cluster_map)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    
    validated_clusters = {}
    next_id = next_single_label
    
    for cluster_id, paths in cluster_map.items():
        paths_list = list(paths)
        
        # –î–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–∑ 2+ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–π –ø—Ä–∏–º–µ–Ω—è–µ–º –º—è–≥–∫—É—é –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—é
        if len(paths_list) >= 2:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥ 0.8 (–≤–º–µ—Å—Ç–æ 0.6)
            # –≠—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç cosine distance ~0.45 –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            if verify_cluster_similarity(paths_list, threshold=0.8):
                validated_clusters[cluster_id] = paths
                print(f"  ‚úÖ –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} –≤–∞–ª–∏–¥–µ–Ω ({len(paths)} —Ñ–æ—Ç–æ)")
            else:
                # –î–∞–∂–µ –µ—Å–ª–∏ —Å—Ç—Ä–æ–≥–∞—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–µ –ø—Ä–æ—à–ª–∞, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä
                # —Ç–∞–∫ –∫–∞–∫ HDBSCAN + –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —É–∂–µ —Å–¥–µ–ª–∞–ª–∏ —Ö–æ—Ä–æ—à—É—é —Ä–∞–±–æ—Ç—É
                print(f"  ‚ö†Ô∏è –ö–ª–∞—Å—Ç–µ—Ä {cluster_id} —Ç—Ä–µ–±—É–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ ({len(paths)} —Ñ–æ—Ç–æ)")
                validated_clusters[cluster_id] = paths  # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä
        else:
            # –û–¥–∏–Ω–æ—á–Ω—ã–µ —Ñ–æ—Ç–æ –≤—Å–µ–≥–¥–∞ –≤–∞–ª–∏–¥–Ω—ã
            validated_clusters[cluster_id] = paths
            print(f"  ‚úÖ –û–¥–∏–Ω–æ—á–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä {cluster_id} ({len(paths)} —Ñ–æ—Ç–æ)")
            next_id += 1
    
    # –û–±–Ω–æ–≤–ª—è–µ–º cluster_map –∏ cluster_by_img
    cluster_map = validated_clusters
    cluster_by_img = defaultdict(set)
    for cluster_id, paths in cluster_map.items():
        for path in paths:
            cluster_by_img[path].add(cluster_id)
    
    print(f"üîç [CLUSTER] –ü–æ—Å–ª–µ –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏: {len(cluster_map)} –≤–∞–ª–∏–¥–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    
    # –≠—Ç–∞–ø 4: –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
    if progress_callback:
        progress_callback("üîÑ –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è...", 95)
    
    plan = []
    for path in all_images:
        clusters = cluster_by_img.get(path)
        if not clusters:
            continue
        plan.append({
            "path": str(path),
            "cluster": sorted(list(clusters)),
            "faces": img_face_count.get(path, 0)
        })

    # –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ
    if progress_callback:
        progress_callback(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ù–∞–π–¥–µ–Ω–æ {len(cluster_map)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(plan)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", 100)

    print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {input_dir} ‚Üí –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(cluster_map)}, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(plan)}")

    # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    processing_time = time.time() - start_time
    print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.1f}—Å")
    
    return {
        "clusters": {
            int(k): [str(p) for p in sorted(v, key=lambda x: str(x))]
            for k, v in cluster_map.items()
        },
        "plan": plan,
        "unreadable": [str(p) for p in unreadable],
        "no_faces": [str(p) for p in no_faces],
    }

def distribute_to_folders(plan: dict, base_dir: Path, cluster_start: int = 1, progress_callback=None) -> Tuple[int, int, int]:
    moved, copied = 0, 0
    moved_paths = set()

    used_clusters = sorted({c for item in plan.get("plan", []) for c in item["cluster"]})
    cluster_id_map = {old: cluster_start + idx for idx, old in enumerate(used_clusters)}

    plan_items = plan.get("plan", [])
    total_items = len(plan_items)
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
    cluster_file_counts = {}
    for item in plan_items:
        clusters = [cluster_id_map[c] for c in item["cluster"]]
        for cluster_id in clusters:
            cluster_file_counts[cluster_id] = cluster_file_counts.get(cluster_id, 0) + 1
    
    if progress_callback:
        progress_callback(f"üîÑ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {total_items} —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–∞–ø–∫–∞–º...", 0)

    for i, item in enumerate(plan_items):
        if progress_callback:
            percent = int((i + 1) / max(total_items, 1) * 100)
            progress_callback(f"üìÅ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤: {percent}% ({i+1}/{total_items})", percent)
            
        src = Path(item["path"])
        clusters = [cluster_id_map[c] for c in item["cluster"]]
        if not src.exists():
            continue

        if len(clusters) == 1:
            cluster_id = clusters[0]
            dst = base_dir / f"{cluster_id}" / src.name
            dst.parent.mkdir(parents=True, exist_ok=True)
            try:
                # Skip if source and destination are the same file
                try:
                    if src.resolve() == dst.resolve():
                        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø—É—Ç–∏): {src} ‚Üí {dst}")
                        continue
                except Exception:
                    if str(src) == str(dst):
                        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏): {src} ‚Üí {dst}")
                        continue
                shutil.move(str(src), str(dst))
                moved += 1
                moved_paths.add(src.parent)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏—è {src} ‚Üí {dst}: {e}")
        else:
            for cluster_id in clusters:
                dst = base_dir / f"{cluster_id}" / src.name
                dst.parent.mkdir(parents=True, exist_ok=True)
                try:
                    # Skip if source and destination are the same file
                    try:
                        if src.resolve() == dst.resolve():
                            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø—É—Ç–∏): {src} ‚Üí {dst}")
                            continue
                    except Exception:
                        if str(src) == str(dst):
                            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Å—Ç—Ä–æ–∫–∏): {src} ‚Üí {dst}")
                            continue
                    shutil.copy2(str(src), str(dst))
                    copied += 1
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è {src} ‚Üí {dst}: {e}")
            try:
                src.unlink()  # —É–¥–∞–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –ø–æ—Å–ª–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞–ø–æ–∫
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è {src}: {e}")

    # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫ —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∞–π–ª–æ–≤
    if progress_callback:
        progress_callback("üìù –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ñ–∞–π–ª–æ–≤...", 95)
    
    for cluster_id, file_count in cluster_file_counts.items():
        old_folder = base_dir / str(cluster_id)
        new_folder = base_dir / f"{cluster_id} ({file_count})"
        
        if old_folder.exists() and old_folder.is_dir():
            try:
                old_folder.rename(new_folder)
                print(f"üìÅ –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–æ: {old_folder.name} ‚Üí {new_folder.name}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏—è {old_folder} ‚Üí {new_folder}: {e}")

    # –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫
    if progress_callback:
        progress_callback("üßπ –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫...", 100)

    for p in sorted(moved_paths, key=lambda x: len(str(x)), reverse=True):
        try:
            if p.exists() and not any(p.iterdir()):
                p.rmdir()
        except Exception:
            pass

    print(f"üì¶ –ü–µ—Ä–µ–º–µ—â–µ–Ω–æ: {moved}, —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {copied}")
    return moved, copied, cluster_start + len(used_clusters)

def find_common_folders_recursive(root_dir: Path):
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –Ω–∞–π—Ç–∏ –≤—Å–µ –ø–∞–ø–∫–∏ '–æ–±—â–∏–µ' –≤ –¥–µ—Ä–µ–≤–µ –∫–∞—Ç–∞–ª–æ–≥–æ–≤"""
    excluded_names = ["–æ–±—â–∏–µ", "–æ–±—â–∞—è", "common", "shared", "–≤—Å–µ", "all", "mixed", "—Å–º–µ—à–∞–Ω–Ω—ã–µ"]
    common_folders = []
    
    print(f"üîç –ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –ø–∞–ø–æ–∫ '–æ–±—â–∏–µ' –≤: {root_dir}")
    print(f"üîç –ò—â–µ–º –ø–∞–ø–∫–∏ —Å –∏–º–µ–Ω–∞–º–∏: {excluded_names}")
    
    def scan_directory(dir_path, level=0):
        indent = "  " * level
        try:
            print(f"{indent}üìÅ –°–∫–∞–Ω–∏—Ä—É–µ–º: {dir_path}")
            for item in dir_path.iterdir():
                if item.is_dir():
                    print(f"{indent}  üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–ø–∫—É: {item.name}")
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–∞ –ø–∞–ø–∫–∞ "–æ–±—â–µ–π"
                    item_name_lower = item.name.lower()
                    for ex in excluded_names:
                        if ex in item_name_lower:
                            common_folders.append(item)
                            print(f"{indent}  ‚úÖ –ù–∞–π–¥–µ–Ω–∞ –ø–∞–ø–∫–∞ '–æ–±—â–∏–µ': {item}")
                            break
                    else:
                        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Å–∫–∞–Ω–∏—Ä—É–µ–º –ø–æ–¥–ø–∞–ø–∫–∏ (—Ç–æ–ª—å–∫–æ –¥–æ —É—Ä–æ–≤–Ω—è 3)
                        if level < 3:
                            scan_directory(item, level + 1)
        except PermissionError:
            print(f"{indent}‚ùå –ù–µ—Ç –¥–æ—Å—Ç—É–ø–∞ –∫ –ø–∞–ø–∫–µ: {dir_path}")
        except Exception as e:
            print(f"{indent}‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è {dir_path}: {e}")
    
    scan_directory(root_dir)
    print(f"üîç –ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(common_folders)} –ø–∞–ø–æ–∫ '–æ–±—â–∏–µ': {[str(f) for f in common_folders]}")
    return common_folders


def process_common_folder_at_level(common_dir: Path, progress_callback=None):
    """
    –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ–¥–Ω—É –ø–∞–ø–∫—É '–æ–±—â–∏–µ':
    1. –ù–∞–π—Ç–∏ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π –Ω–∞ —Ñ–æ—Ç–æ
    2. –°–æ–∑–¥–∞—Ç—å –ø–∞–ø–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ (–¥–∞–∂–µ –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ –æ–±—â–∏—Ö —Ñ–æ—Ç–æ)
    3. –°–æ–∑–¥–∞—Ç—å 2 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏
    4. –ù–ï —Ç—Ä–æ–≥–∞—Ç—å —Å–∞–º–∏ –æ–±—â–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ - –æ–Ω–∏ –æ—Å—Ç–∞—é—Ç—Å—è –≤ –ø–∞–ø–∫–µ '–æ–±—â–∏–µ'
    """
    parent_dir = common_dir.parent
    
    print(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–ø–∫—É '–æ–±—â–∏–µ': {common_dir}")
    print(f"üîç –†–æ–¥–∏—Ç–µ–ª—å—Å–∫–∞—è –ø–∞–ø–∫–∞: {parent_dir}")
    
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º –¢–û–õ–¨–ö–û —Ñ–æ—Ç–æ –∏–∑ –ø–∞–ø–∫–∏ "–æ–±—â–∏–µ" —á—Ç–æ–±—ã –Ω–∞–π—Ç–∏ –≤—Å–µ—Ö –ª—é–¥–µ–π
    print(f"üîç –í—ã–∑—ã–≤–∞–µ–º build_plan_live –¥–ª—è: {common_dir}")
    data = build_plan_live(common_dir, include_excluded=True, progress_callback=progress_callback)
    plan = data.get('plan', [])
    
    print(f"üîç –ü–æ–ª—É—á–µ–Ω –ø–ª–∞–Ω —Å {len(plan)} —Ñ–∞–π–ª–∞–º–∏")
    if plan:
        print(f"üîç –ü–µ—Ä–≤—ã–µ —Ñ–∞–π–ª—ã –≤ –ø–ª–∞–Ω–µ: {[item['path'] for item in plan[:3]]}")
    
    if not plan:
        print(f"‚ùå –ù–µ—Ç —Ñ–æ—Ç–æ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ {common_dir}")
        return 0
    
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ ID –ø–∞–ø–æ–∫ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    existing_ids = set()
    for d in parent_dir.iterdir():
        if d.is_dir():
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —á–∏—Å–ª–æ –∏–∑ –Ω–∞—á–∞–ª–∞ –∏–º–µ–Ω–∏ –ø–∞–ø–∫–∏
                id_str = d.name.split(' ')[0].split('-')[0].split('_')[0]
                if id_str.isdigit():
                    existing_ids.add(int(id_str))
            except:
                continue
    
    print(f"üîç –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ ID –ø–∞–ø–æ–∫: {sorted(existing_ids)}")
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ ID –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (–ª—é–¥–µ–π) –∏–∑ –æ–±—â–∏—Ö —Ñ–æ—Ç–æ
    cluster_ids = set()
    for item in plan:
        for cid in item['cluster']:
            cluster_ids.add(cid)
    
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ª—é–¥–µ–π –Ω–∞ –æ–±—â–∏—Ö —Ñ–æ—Ç–æ: {len(cluster_ids)}")
    print(f"üîç ID –ª—é–¥–µ–π: {sorted(cluster_ids)}")
    
    created = 0
    
    # –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞, –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–µ—Ç –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–∞–ø–∫–∞—Ö
    for cluster_id in sorted(cluster_ids):
        if cluster_id not in existing_ids:
            folder = parent_dir / str(cluster_id)
            folder.mkdir(parents=True, exist_ok=True)
            print(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ –¥–ª—è —á–µ–ª–æ–≤–µ–∫–∞ {cluster_id}: {folder}")
            created += 1
            existing_ids.add(cluster_id)
        else:
            print(f"‚è© –ü–∞–ø–∫–∞ –¥–ª—è —á–µ–ª–æ–≤–µ–∫–∞ {cluster_id} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ ID (—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ + —Å–æ–∑–¥–∞–Ω–Ω—ã–µ)
    all_ids = existing_ids.union(cluster_ids)
    
    # –°–æ–∑–¥–∞—ë–º 2 –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Å—Ç—ã–µ –ø–∞–ø–∫–∏ —Å –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏–∏
    max_id = max(all_ids) if all_ids else 0
    print(f"üîç –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π ID: {max_id}")
    
    for i in range(1, 3):
        new_id = max_id + i
        folder = parent_dir / str(new_id)
        folder.mkdir(parents=True, exist_ok=True)
        print(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—É—Å—Ç–∞—è –ø–∞–ø–∫–∞ {i}/2: {folder}")
        created += 1
    
    print(f"‚úÖ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ –ø–∞–ø–æ–∫: {created}")
    print(f"üì∏ –û–±—â–∏–µ —Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏–∏ –æ—Å—Ç–∞–ª–∏—Å—å –Ω–µ—Ç—Ä–æ–Ω—É—Ç—ã–º–∏ –≤: {common_dir}")
    
    return created


def process_group_folder(group_dir: Path, progress_callback=None, include_excluded: bool = False):
    """
    –ï—Å–ª–∏ include_excluded=True, —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏—â–µ–º –≤—Å–µ –ø–∞–ø–∫–∏ "–æ–±—â–∏–µ" –∏ –∫–æ–ø–∏—Ä—É–µ–º —Ñ–æ—Ç–æ –≤ –ø–∞–ø–∫–∏ –ª—é–¥–µ–π.
    –ò–Ω–∞—á–µ - –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –ø–æ–¥–ø–∞–ø–∫—É –æ—Ç–¥–µ–ª—å–Ω–æ.
    """
    cluster_counter = 1
    
    import time
    call_id = int(time.time() * 1000) % 10000
    print(f"üîç process_group_folder [{call_id}] –≤—ã–∑–≤–∞–Ω–∞ –¥–ª—è: {group_dir}, include_excluded={include_excluded}")
    
    if include_excluded:
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ –ø–∞–ø–∫–∏ "–æ–±—â–∏–µ"
        if progress_callback:
            progress_callback("üîç –ü–æ–∏—Å–∫ –ø–∞–ø–æ–∫ '–æ–±—â–∏–µ' –≤–æ –≤—Å–µ–π –∏–µ—Ä–∞—Ä—Ö–∏–∏...", 10)
        
        common_folders = find_common_folders_recursive(group_dir)
        
        if not common_folders:
            if progress_callback:
                progress_callback("‚ùå –ü–∞–ø–∫–∏ '–æ–±—â–∏–µ' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤–æ –≤—Å–µ–π –∏–µ—Ä–∞—Ä—Ö–∏–∏", 100)
            print(f"‚ùå –ü–∞–ø–∫–∏ '–æ–±—â–∏–µ' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {group_dir}")
            print(f"üîç –ü—Ä–æ–≤–µ—Ä–∏–ª–∏ —Å–ª–µ–¥—É—é—â–∏–µ –ø–∞–ø–∫–∏:")
            
            def debug_scan_directory(dir_path, level=0):
                indent = "  " * level
                try:
                    print(f"{indent}üìÅ {dir_path}")
                    for item in dir_path.iterdir():
                        if item.is_dir():
                            print(f"{indent}  ‚îî‚îÄ‚îÄ üìÅ {item.name}")
                            if level < 2:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≥–ª—É–±–∏–Ω—É
                                debug_scan_directory(item, level + 1)
                except Exception as e:
                    print(f"{indent}  ‚ùå –û—à–∏–±–∫–∞: {e}")
            
            debug_scan_directory(group_dir)
            return 0, 0, cluster_counter
        
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(common_folders)} –ø–∞–ø–æ–∫ '–æ–±—â–∏–µ'")
        
        total_copied = 0
        total_folders = len(common_folders)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –Ω–∞–π–¥–µ–Ω–Ω—É—é –ø–∞–ø–∫—É "–æ–±—â–∏–µ"
        for i, common_folder in enumerate(common_folders):
            if progress_callback:
                percent = 20 + int((i + 1) / total_folders * 70)
                progress_callback(f"üìã –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞–ø–∫—É: {common_folder.name} ({i+1}/{total_folders})", percent)
            
            copied = process_common_folder_at_level(common_folder, progress_callback)
            total_copied += copied
        
        if progress_callback:
            progress_callback(f"‚úÖ –í—Å–µ–≥–æ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {total_copied} —Ñ–∞–π–ª–æ–≤", 100)
        
        print(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—â–∏—Ö —Ñ–æ—Ç–æ [{call_id}] –∑–∞–≤–µ—Ä—à–µ–Ω–∞: —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ {total_copied} —Ñ–∞–π–ª–æ–≤ –∏–∑ {len(common_folders)} –ø–∞–ø–æ–∫")
        return 0, total_copied, cluster_counter
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –ø–æ–¥–ø–∞–ø–∫—É, –∏—Å–∫–ª—é—á–∞—è –ø–∞–ø–∫–∏ '–æ–±—â–∏–µ'
    subfolders = [f for f in sorted(group_dir.iterdir()) if f.is_dir() and "–æ–±—â–∏–µ" not in f.name.lower()]
    total_subfolders = len(subfolders)
    for i, subfolder in enumerate(subfolders):
        if progress_callback:
            percent = 10 + int((i + 1) / max(total_subfolders, 1) * 80)
            progress_callback(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ–¥–ø–∞–ø–∫–∞: {subfolder.name} ({i+1}/{total_subfolders})", percent)
        print(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–æ–¥–ø–∞–ø–∫–∞ [{call_id}]: {subfolder}")
        plan = build_plan_live(subfolder, progress_callback=progress_callback)
        print(f"üìä –ö–ª–∞—Å—Ç–µ—Ä–æ–≤: {len(plan.get('clusters', {}))}, —Ñ–∞–π–ª–æ–≤: {len(plan.get('plan', []))}")
        moved, copied, _ = distribute_to_folders(
            plan, subfolder, cluster_start=1, progress_callback=progress_callback
        )



