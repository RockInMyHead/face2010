"""
–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –±–µ–∑ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.
"""
import os
import time
from pathlib import Path
from typing import List, Dict, Tuple
import numpy as np
from collections import defaultdict
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import pairwise_distances
from sklearn.preprocessing import normalize
import cv2
from PIL import Image

# –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}

def is_image(path: Path) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º."""
    return path.suffix.lower() in IMG_EXTS

def imread_safe(path: Path) -> np.ndarray:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
    try:
        if path.suffix.lower() in {'.jpg', '.jpeg', '.png'}:
            img = cv2.imread(str(path))
            if img is not None:
                return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        return None
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {path.name}: {e}")
        return None

def detect_faces_simple(img: np.ndarray) -> List[Dict]:
    """–ü—Ä–æ—Å—Ç–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü —Å –ø–æ–º–æ—â—å—é OpenCV."""
    try:
        print(f"üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–º: {img.shape}")
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ grayscale –¥–ª—è –¥–µ—Ç–µ–∫—Ü–∏–∏
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Å–∫–∞–¥ –•–∞–∞—Ä–∞
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        print(f"üéØ –ù–∞–π–¥–µ–Ω–æ –ª–∏—Ü: {len(faces)}")
        
        results = []
        for (x, y, w, h) in faces:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ª–∏—Ü–æ
            face_img = img[y:y+h, x:x+w]
            
            # –ü—Ä–æ—Å—Ç–æ–µ "—ç–º–±–µ–¥–¥–∏–Ω–≥" - —Å—Ä–µ–¥–Ω–∏–π —Ü–≤–µ—Ç –ª–∏—Ü–∞
            embedding = np.mean(face_img.reshape(-1, 3), axis=0)
            embedding = embedding / np.linalg.norm(embedding)  # L2 normalize
            
            results.append({
                'embedding': embedding,
                'quality': 0.8,  # –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
                'bbox': (x, y, w, h)
            })
        
        return results
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–∏ –ª–∏—Ü: {e}")
        return []

def merge_similar_clusters(embeddings: np.ndarray, labels: np.ndarray, merge_threshold: float = 0.4) -> np.ndarray:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤."""
    unique_labels = np.unique(labels)
    if len(unique_labels) <= 1:
        return labels
    
    # –í—ã—á–∏—Å–ª—è–µ–º —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
    centroids = {}
    for label in unique_labels:
        mask = labels == label
        if np.sum(mask) > 0:
            centroid = np.mean(embeddings[mask], axis=0)
            centroids[label] = centroid / np.linalg.norm(centroid)
    
    # –ù–∞—Ö–æ–¥–∏–º –ø–∞—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è —Å–ª–∏—è–Ω–∏—è
    merged_labels = labels.copy()
    label_mapping = {label: label for label in unique_labels}
    
    for i, label1 in enumerate(unique_labels):
        if label1 not in centroids:
            continue
            
        for j, label2 in enumerate(unique_labels[i+1:], i+1):
            if label2 not in centroids:
                continue
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞–º–∏
            cosine_dist = 1 - np.dot(centroids[label1], centroids[label2])
            
            if cosine_dist < merge_threshold:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã
                target_label = min(label1, label2)
                source_label = max(label1, label2)
                
                # –û–±–Ω–æ–≤–ª—è–µ–º mapping
                for old_label, new_label in label_mapping.items():
                    if new_label == source_label:
                        label_mapping[old_label] = target_label
                
                print(f"üîó –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã {label1} –∏ {label2} (—Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ: {cosine_dist:.3f})")
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º mapping
    for i, label in enumerate(labels):
        merged_labels[i] = label_mapping[label]
    
    return merged_labels

def merge_single_clusters(embeddings: np.ndarray, labels: np.ndarray, merge_threshold: float = 0.6) -> np.ndarray:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã —Å –±–ª–∏–∂–∞–π—à–∏–º–∏, –∏—Å–ø–æ–ª—å–∑—É—è –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Ç—Ä–∏–∫."""
    unique_labels = np.unique(labels)
    if len(unique_labels) <= 1:
        return labels
    
    # –ù–∞—Ö–æ–¥–∏–º —Ä–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    cluster_sizes = {}
    for label in unique_labels:
        cluster_sizes[label] = np.sum(labels == label)
    
    # –ù–∞—Ö–æ–¥–∏–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    single_clusters = [label for label, size in cluster_sizes.items() if size == 1]
    
    if not single_clusters:
        return labels
    
    merged_labels = labels.copy()
    
    for single_label in single_clusters:
        # –ù–∞—Ö–æ–¥–∏–º –∏–Ω–¥–µ–∫—Å –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
        single_idx = np.where(labels == single_label)[0][0]
        single_embedding = embeddings[single_idx]
        
        # –ò—â–µ–º –±–ª–∏–∂–∞–π—à–∏–π –∫–ª–∞—Å—Ç–µ—Ä —Å —É—á–µ—Ç–æ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫
        best_cluster = None
        best_score = float('inf')
        
        for other_label in unique_labels:
            if other_label == single_label or cluster_sizes[other_label] == 1:
                continue
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞ –¥—Ä—É–≥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
            other_mask = labels == other_label
            other_embeddings = embeddings[other_mask]
            other_centroid = np.mean(other_embeddings, axis=0)
            other_centroid = other_centroid / np.linalg.norm(other_centroid)
            
            # –ú–µ—Ç—Ä–∏–∫–∞ 1: –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            cosine_dist = 1 - np.dot(single_embedding, other_centroid)
            
            # –ú–µ—Ç—Ä–∏–∫–∞ 2: L2 —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ)
            l2_dist = np.linalg.norm(single_embedding - other_centroid)
            
            # –ú–µ—Ç—Ä–∏–∫–∞ 3: –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –ª—é–±–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞
            min_dist_to_any = float('inf')
            for other_emb in other_embeddings:
                other_emb_norm = other_emb / np.linalg.norm(other_emb)
                dist_to_element = 1 - np.dot(single_embedding, other_emb_norm)
                min_dist_to_any = min(min_dist_to_any, dist_to_element)
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞)
            combined_score = (
                0.5 * cosine_dist +           # –û—Å–Ω–æ–≤–Ω–∞—è –º–µ—Ç—Ä–∏–∫–∞
                0.3 * l2_dist +               # L2 —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
                0.2 * min_dist_to_any        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            )
            
            # –°–º—è–≥—á–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥: —É—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ü–µ–ª–µ–≤–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
            size_factor = 1.0 + 0.1 * cluster_sizes[other_label]  # –ë–æ–ª—å—à–µ –∫–ª–∞—Å—Ç–µ—Ä—ã = –º—è–≥—á–µ –ø–æ—Ä–æ–≥
            adjusted_threshold = merge_threshold * size_factor
            
            if combined_score < best_score and combined_score < adjusted_threshold:
                best_score = combined_score
                best_cluster = other_label
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –±–ª–∏–∂–∞–π—à–∏–º –∫–ª–∞—Å—Ç–µ—Ä–æ–º
        if best_cluster is not None:
            merged_labels[single_idx] = best_cluster
            print(f"üîó –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä {single_label} —Å {best_cluster} (–æ—Ü–µ–Ω–∫–∞: {best_score:.3f})")
    
    return merged_labels

def build_plan_simple(
    input_dir: Path,
    n_clusters: int = 8,
    progress_callback=None
) -> Dict:
    """–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±–µ–∑ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π."""
    print(f"üöÄ [SIMPLE] –ó–∞–ø—É—Å–∫ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {input_dir}")
    
    input_dir = Path(input_dir)
    start_time = time.time()
    
    # –°–æ–±–∏—Ä–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
    all_images = [p for p in input_dir.rglob("*") if is_image(p)]
    print(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(all_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    if progress_callback:
        progress_callback(f"üìÇ –ù–∞–π–¥–µ–Ω–æ {len(all_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", 5)
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    all_embeddings = []
    owners = []
    img_face_count = {}
    unreadable = []
    no_faces = []
    
    total = len(all_images)
    
    for i, img_path in enumerate(all_images):
        if progress_callback and i % 5 == 0:
            percent = 10 + int((i + 1) / max(total, 1) * 70)
            progress_callback(f"üì∑ –ê–Ω–∞–ª–∏–∑: {percent}% ({i+1}/{total})", percent)
        
        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ
        img = imread_safe(img_path)
        if img is None:
            unreadable.append(img_path)
            continue
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ü
        try:
            faces = detect_faces_simple(img)
            
            if not faces:
                print(f"‚ö†Ô∏è –õ–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {img_path.name}")
                no_faces.append(img_path)
                continue
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            img_face_count[img_path] = len(faces)
            
            for face in faces:
                all_embeddings.append(face['embedding'])
                owners.append(img_path)
                
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {img_path.name}: {e}")
            unreadable.append(img_path)
    
    if not all_embeddings:
        print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ª–∏—Ü –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        return {
            "clusters": {},
            "plan": [],
            "unreadable": [str(p) for p in unreadable],
            "no_faces": [str(p) for p in no_faces],
        }
    
    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ {len(all_embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ {len(set(owners))} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
    
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    if progress_callback:
        progress_callback(f"üîÑ Agglomerative Clustering {len(all_embeddings)} –ª–∏—Ü...", 85)
    
    print("‚öôÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º AgglomerativeClustering —Å –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –º–µ—Ç—Ä–∏–∫–æ–π")
    X = np.vstack(all_embeddings)
    
    # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –∫–æ—Å–∏–Ω—É—Å–Ω—ã–µ
    dist_matrix = pairwise_distances(X, metric='cosine')
    clustering = AgglomerativeClustering(
        n_clusters=n_clusters,
        metric='precomputed',
        linkage='average'
    )
    labels = clustering.fit_predict(dist_matrix)
    
    print(f"‚úÖ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(set(labels))} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    
    # –û—Ç–∫–ª—é—á–∞–µ–º —Å–ª–∏—è–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –ª—é–¥–µ–π
    # labels = merge_similar_clusters(X, labels, merge_threshold=0.1)
    # labels = merge_single_clusters(X, labels, merge_threshold=0.2)
    
    print(f"‚úÖ –ü–æ—Å–ª–µ —Å–ª–∏—è–Ω–∏—è: {len(set(labels))} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    cluster_map = defaultdict(set)
    cluster_by_img = defaultdict(set)
    
    for idx, (label, path) in enumerate(zip(labels, owners)):
        cluster_map[label].add(path)
        cluster_by_img[path].add(label)
    
    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–ª–∞–Ω
    plan = []
    for path in all_images:
        clusters = cluster_by_img.get(path)
        if not clusters:
            continue
        plan.append({
            "path": str(path),
            "cluster": sorted(list(clusters)),
            "faces": img_face_count.get(path, 0)
        })
    
    processing_time = time.time() - start_time
    print(f"‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {processing_time:.1f}—Å")
    
    if progress_callback:
        progress_callback(f"‚úÖ –ì–æ—Ç–æ–≤–æ! {len(cluster_map)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤", 100)
    
    return {
        "clusters": {str(k): [str(p) for p in v] for k, v in cluster_map.items()},
        "plan": plan,
        "unreadable": [str(p) for p in unreadable],
        "no_faces": [str(p) for p in no_faces],
    }

def distribute_to_folders(plan: dict, base_dir: Path, cluster_start: int = 1, progress_callback=None) -> Tuple[int, int, int]:
    """–†–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ñ–∞–π–ª—ã –ø–æ –ø–∞–ø–∫–∞–º —Å–æ–≥–ª–∞—Å–Ω–æ –ø–ª–∞–Ω—É."""
    import shutil
    
    moved, copied = 0, 0
    moved_paths = set()

    used_clusters = sorted({c for item in plan.get("plan", []) for c in item["cluster"]})
    cluster_id_map = {old: cluster_start + idx for idx, old in enumerate(used_clusters)}
    plan_items = plan.get("plan", [])
    total_items = len(plan_items)
    if progress_callback:
        progress_callback(f"üîÑ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {total_items} —Ñ–∞–π–ª–æ–≤ –ø–æ –ø–∞–ø–∫–∞–º...", 0)

    cluster_file_counts = {}
    for item in plan_items:
        clusters = [cluster_id_map[c] for c in item["cluster"]]
        for cid in clusters:
            cluster_file_counts[cid] = cluster_file_counts.get(cid, 0) + 1

    for i, item in enumerate(plan_items):
        if progress_callback:
            percent = int((i + 1) / max(total_items, 1) * 100)
            progress_callback(f"üìÅ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤: {percent}% ({i+1}/{total_items})", percent)
        src = Path(item["path"]);
        clusters = [cluster_id_map[c] for c in item["cluster"]]
        if not src.exists():
            continue
        if len(clusters) == 1:
            dst = base_dir / f"{clusters[0]}" / src.name
            dst.parent.mkdir(parents=True, exist_ok=True)
            if src.resolve() != dst.resolve(): shutil.move(str(src), str(dst)); moved+=1; moved_paths.add(src.parent)
        else:
            for cid in clusters:
                dst = base_dir / f"{cid}" / src.name; dst.parent.mkdir(parents=True, exist_ok=True)
                if src.resolve() != dst.resolve(): shutil.copy2(str(src), str(dst)); copied+=1
            try: src.unlink()
            except: pass
    if progress_callback:
        progress_callback("üìù –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ñ–∞–π–ª–æ–≤...", 95)
    for cid, cnt in cluster_file_counts.items():
        old_folder = base_dir / str(cid); new_folder = base_dir / f"{cid} ({cnt})"
        if old_folder.exists():
            try: old_folder.rename(new_folder)
            except: pass
    if progress_callback:
        progress_callback("üßπ –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö –ø–∞–ø–æ–∫...", 100)
    for p in sorted(moved_paths, key=lambda x: len(str(x)), reverse=True):
        try: p.rmdir()
        except: pass
    print(f"üì¶ –ü–µ—Ä–µ–º–µ—â–µ–Ω–æ: {moved}, —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ: {copied}")
    return moved, copied, cluster_start + len(used_clusters)

def process_group_folder(group_dir: Path, progress_callback=None, include_excluded: bool = False):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≥—Ä—É–ø–ø—É –ø–∞–ø–æ–∫."""
    cluster_counter = 1
    common = []
    if include_excluded:
        common = find_common_folders_recursive(group_dir)
        total = len(common)
        for i, c in enumerate(common):
            if progress_callback: progress_callback(f"üìã –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—â–∏—Ö —Ñ–æ—Ç–æ {i+1}/{total}", 10+int(i/total*70))
            process_common_folder_at_level(c, progress_callback)
        return 0, sum(1 for c in common), cluster_counter
    subdirs = [d for d in sorted(group_dir.iterdir()) if d.is_dir()]
    total = len(subdirs)
    moved_all, copied_all = 0, 0
    for i, sub in enumerate(subdirs):
        if progress_callback: progress_callback(f"üîç –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è {sub.name} ({i+1}/{total})", 10+int(i/total*70))
        data = build_plan_simple(
            input_dir=sub,
            n_clusters=3,
            progress_callback=progress_callback
        )
        m, c, _ = distribute_to_folders(data, sub, cluster_start=1, progress_callback=progress_callback)
        moved_all+=m; copied_all+=c
    return moved_all, copied_all, cluster_counter

def find_common_folders_recursive(group_dir: Path):
    """–ù–∞—Ö–æ–¥–∏—Ç –æ–±—â–∏–µ –ø–∞–ø–∫–∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ."""
    excluded_names = ["–æ–±—â–∏–µ", "–æ–±—â–∞—è", "common", "shared", "–≤—Å–µ", "all", "mixed", "—Å–º–µ—à–∞–Ω–Ω—ã–µ"]
    common = []
    for subdir in group_dir.iterdir():
        if subdir.is_dir() and any(ex in subdir.name.lower() for ex in excluded_names):
            common.append(subdir)
    return common

def process_common_folder_at_level(common_dir: Path, progress_callback=None):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–±—â—É—é –ø–∞–ø–∫—É –Ω–∞ —É—Ä–æ–≤–Ω–µ."""
    # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –æ–±—â–∏—Ö –ø–∞–ø–æ–∫
    pass
